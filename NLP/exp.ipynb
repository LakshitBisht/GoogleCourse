{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIM\n",
    "Apply different tokenization techniques using NLTK."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory\n",
    "NLP: NLP stands for Natural Language Processing, which is a part of Computer Science, Human language, and Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction, and topic segmentation.\n",
    "\n",
    "Tokenization: Tokenization is the first step in any NLP pipeline. It has an important effect on the rest of your pipeline. A tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements. The token occurrences in a document can be used directly as a vector representing that document. \n",
    "\n",
    "This immediately turns an unstructured string (text document) into a numerical data structure suitable for machine learning. They can also be used directly by a computer to trigger useful actions and responses. Or they might be used in a machine learning pipeline as features that trigger more complex decisions or behavior.\n",
    "\n",
    "https://neptune.ai/blog/tokenization-in-nlp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Requirements:\n",
    "1. Code Editor(Jupyter Notebook)\n",
    "2. Python kernel\n",
    "3. Working PC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lakshit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize,wordpunct_tokenize,TreebankWordTokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Paragaraph to Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in any NLP pipeline.', '#Tokenization #Lakshit-Bisht #20BCS6588']\n"
     ]
    }
   ],
   "source": [
    "text = 'Tokenization is the first step in any NLP pipeline. #Tokenization #Lakshit-Bisht #20BCS6588'\n",
    "sentences= sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Paragaraph to Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'any', 'NLP', 'pipeline', '.', '#', 'Tokenization', '#', 'Lakshit-Bisht', '#', '20BCS6588']\n"
     ]
    }
   ],
   "source": [
    "words= word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Paragraph to Word Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'any', 'NLP', 'pipeline', '.', '#', 'Tokenization', '#', 'Lakshit', '-', 'Bisht', '#', '20BCS6588']\n"
     ]
    }
   ],
   "source": [
    "words= wordpunct_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Treebank Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'any', 'NLP', 'pipeline.', '#', 'Tokenization', '#', 'Lakshit-Bisht', '#', '20BCS6588']\n"
     ]
    }
   ],
   "source": [
    "tokenizer=TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result:\n",
    "Different types of tokenization,like paragraph to sentence, paragraph to word and paragraph to word punctuation, were successfully applied and the results were observed.\n",
    "\n",
    "# Learning Outcomes:\n",
    "1. Understood the pipline of nlp.\n",
    "2. Understood the importance of tokenization in nlp pipeline.\n",
    "3. Applied paragraph to sentence tokenization.\n",
    "4. Applied paragraph to Words and word punctuation tokenization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a99ddddd9707e996f158d5f15667ccf85de0da941d875ebc755d121abccb487d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
